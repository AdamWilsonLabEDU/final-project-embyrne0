---
title: "Unveiling Hidden Patterns: Clustering Analysis with ISMIP6 Models"
author: Eleanor M. Byrne
subtitle: A PCA and Clustering Analysis 
date: today
date-format: long
---

# Introduction

How do different Global Climate Models (GCMs) impact the projections of ice sheet behavior in ISMIP6 simulations? What is the influence of ocean forcing on the projections of ice sheet dynamics in ISMIP6 models? How do different Representative Concentration Pathways (RCPs) affect the projected outcomes of ice sheet models in ISMIP6? These are questions that have been asked within the ice sheet community in the recent years as new data, methods, and ideas have submerged.

The Ice Sheet Model Intercomparison Project for CMIP6 (ISMIP6) aims to address these questions by providing a framework for evaluating and comparing the responses of ice sheet models to various climate scenarios. ISMIP6 integrates multiple GCMs, RCPs, and ocean forcing datasets to project the future behavior of the Greenland and Antarctic ice sheets.

This study focuses on analyzing the RCP experiments and looking at one key variable Surface Mass Balance flux. Principal Component Analysis (PCA) and K-means clustering will be used to analyze the variables, aiming to uncover patterns and groupings in the data. This approach will help to understand the variations in ice sheet response under various scenarios, RCP 8.5 and 2.6. **Hypothesis:** Variations in Representative Concentration Pathways (RCPs) will lead to significantly different projections of Surface Mass Balance (SMB) flux in ISMIP6 simulations. Specifically, higher RCP scenarios will result in greater reductions in a negative SMB fluxes due to increased temperature and melting rates.

# Materials and methods

\[\~ 200 words\]

Narrative: Clear narrative description of the data sources and methods. Includes data from at least two sources that were integrated / merged in R.

Code: The code associated with the project is well organized and easy to follow. Demonstrates mastery of R graphics and functions.

Data:

The data used in this study was sourced from the Center for Computational Research (CCR) at the University at Buffalo. The ISMIP6 dataset was previously downloaded from the CCR repository, focusing on the experiment and variable types needed for the RCP experiment group. This dataset includes projections of land ice thickness and Surface Mass Balance (SMB) flux different Representative Concentration Pathways (RCPs).

[CCR website](https://www.buffalo.edu/ccr.html)

See <http://rmarkdown.rstudio.com/> for all the amazing things you can do.

Refer to output in your narrative like this: x=`r x` .

Load any required packages in a code chunk (you may need to install some packages):

```{r, message=F, warning=F}
# loading needed packages 
# install.packages("tidyverse")
library(tidyverse)

library(leaflet)
library(kableExtra)
library(htmlwidgets)
library(widgetframe)
library(lattice)
library(ncdf4)
library(stringr)
library(dplyr)
# install.packages("corrr")
library('corrr')
# install.packages("FactoMineR")
library("FactoMineR")
# install.packages("ggcorrplot")
library(ggcorrplot)
knitr::opts_chunk$set(widgetframe_widgets_dir = 'widgets' ) 
knitr::opts_chunk$set(cache=TRUE)  # cache the results for quick compiling
```

```{r}
nc_data <- nc_open('C:/Users/Margo/OneDrive/Documents/Fall 2024 Courses/Spatial Data Science Course/final-project-embyrne0/data/lithk_GIS_AWI_ISSM1_exp05.nc')

nc_data # this is an example of what a lithk nc file would look like 
```

## Download and clean all required data

```{r}
file_dir <- "C:/Users/Margo/OneDrive/Documents/final-project-embyrne0/data"

# Variables to analyze
vars <- c("acabf")

# Experiments to analyze
exp <- c("exp05", "exp07")

# Initialize an empty list to store data for each combination of experiment, variable, model group, and model
data_by_group_exp_var <- list()

# Loop through each experiment
for (experiment in exp) {
  
  # Loop through each variable
  for (variable in vars) {
    
    # Define the file search pattern for each combination of experiment and variable
    pattern <- paste0(variable, "_.*_.*_.*_", experiment, ".nc")
    
    # Use list.files with 'fixed = TRUE' to avoid regex issues
    file_list <- list.files(path = file_dir, pattern = pattern, full.names = TRUE, recursive = TRUE)
    
    # Check if there are any files found for this combination
    if (length(file_list) == 0) {
      cat("No files found for variable", variable, "and experiment", experiment, ". Skipping...\n")
      next
    }
    
    # Process the files for this variable and experiment
    for (file in file_list) {
      file_name <- basename(file)
      
      # Use regex to extract model group and model name
      match_result <- str_match(file_name, paste0(variable, "_GIS_(.*)_([^_]+)_", experiment, ".nc"))
      
      if (is.na(match_result[1, 1])) {
        cat("Error matching file name for", file_name, ". Skipping...\n")
        next
      }
      
      model_group <- match_result[2]
      model_name <- match_result[3]
      
      cat("Reading data of model", model_name, "from group", model_group, "from file:", substr(file, nchar(file_dir) + 1, nchar(file)), "\n")
      
      # Open the NetCDF file
      nc_data <- nc_open(file)
      
      # Handle 'time' dimension
      if ("time" %in% names(nc_data$dim)) {
        
        # Extract time values and units
        time_values <- nc_data$dim$time$vals
        time_units <- nc_data$dim$time$units
        
        # Convert the time units to date
        start_date <- as.Date(sub("days since ", "", time_units))  # Extract start date
        time_dates <- start_date + time_values  # Calculate actual dates
        
        # Extract the starting year (first time index)
        first_time_index <- 1
        first_time_date <- time_dates[first_time_index]
        start_year <- format(first_time_date, "%Y")
        
        # Print the starting year
        cat("Starting year for", file_name, ":", start_year, "\n")
        
      } else {
        cat("Time variable 'time' not found in file", file, "- skipping\n")
        nc_close(nc_data)
        next
      }
      
      # Now handle the dimension check
      # Extract the variable of interest (e.g., 'lithk', 'litemptop', etc.)
      var_data <- ncvar_get(nc_data, variable)
      
      # Check if var_data is NULL or empty
      if (is.null(var_data) || length(var_data) == 0) {
        cat("Error: Variable", variable, "in file", file_name, "is empty or not found. Skipping...\n")
        nc_close(nc_data)
        next
      }
      
      # Print dimensions of the variable data for debugging
      cat("Dimensions of", variable, ":", dim(var_data), "\n")
      
      # Check the dimensions of the variable
      var_dims <- dim(var_data)
      num_time_steps <- var_dims[3]  # Time is the 3rd dimension, adjust if necessary
      
      # Ensure the length of var_data matches the expected number of cells for each time step
      expected_length_per_time_step <- 337 * 577  # Number of cells per time step (longitude x latitude)
      expected_length_total <- expected_length_per_time_step * num_time_steps
      
      # Flatten the variable data for analysis
      var_1d <- as.vector(var_data)
      
      # Print length of var_1d for debugging
      cat("Length of var_1d:", length(var_1d), "\n")
      
      # Check if the length matches the expected length
      if (!is.na(length(var_1d)) && length(var_1d) > 0) {
        
        # Continue with processing the data as needed
        cat("Variable", variable, "has the expected dimensions and will be processed.\n")
        
        
        # Create a key that combines experiment, variable, model group, and model name
        group_exp_var_key <- paste0(model_group, "_", model_name, "_", experiment, "_", variable)
        
        # If the key doesn't exist, initialize it
        if (!(group_exp_var_key %in% names(data_by_group_exp_var))) {
          data_by_group_exp_var[[group_exp_var_key]] <- list()
        }
        
        # Append the processed data (var_1d) to the list under the key
        data_by_group_exp_var[[group_exp_var_key]][[length(data_by_group_exp_var[[group_exp_var_key]]) + 1]] <- var_1d
        
      } else {
        cat("Warning: Variable", variable, "does not match the expected dimensions. Skipping...\n")
      }
      
      # Close the NetCDF file
      nc_close(nc_data)
    }
  }
}
```

```{r}

# Getting rid of NaN and zeros 
  # First, convert NaN to 0 and then change to -9999
  # Afterwards drop
# Iterate over the `data_by_group_exp_var` to clean each variable's data in the list.
for (key in names(data_by_group_exp_var)) {
  
  # Iterate over each list entry under the key
  for (i in seq_along(data_by_group_exp_var[[key]])) {
    
    # Get the data (var_1d_clean)
    var_data <- data_by_group_exp_var[[key]][[i]]
    
    # Convert NaN to 0, then change 0 to -9999
    var_data[is.na(var_data)] <- 0
    var_data[var_data == 0] <- -9999
    
    # Remove all -9999 values
    cleaned_data <- var_data[var_data != -9999]
    
    # Update the list with the cleaned data
    data_by_group_exp_var[[key]][[i]] <- cleaned_data
  }
}
```

Add any additional processing steps here.

# Methods

Using the PCA and the K-means Clustering

```{r}
# Initialize empty lists to store data for each experiment
data_exp05 <- list()
data_exp07 <- list()

# Extract and prepare data for each experiment
for (key in names(data_by_group_exp_var)) {
  split_key <- strsplit(key, "_")[[1]]
  model_group <- split_key[1]
  model_name <- split_key[2]
  experiment <- split_key[3]
  variable <- split_key[4]
  
  # Combine all the data for this key into a matrix
  data_matrix <- do.call(cbind, data_by_group_exp_var[[key]])
  
  # Store the data matrix in the respective experiment list
  if (experiment == "exp05") {
    data_exp05[[key]] <- data_matrix
  } else if (experiment == "exp07") {
    data_exp07[[key]] <- data_matrix
  }
}

# Convert the lists to data frames for PCA
df_exp05 <- do.call(rbind, data_exp05)

df_exp07 <- do.call(rbind, data_exp07)
```

```{r}
# Perform PCA on exp05 data
# Perform PCA for exp05
pca_exp05 <- prcomp(df_exp05[, !colnames(df_exp05) %in% "Experiment"], scale = TRUE)

# Perform PCA for exp07
pca_exp07 <- prcomp(df_exp07[, !colnames(df_exp07) %in% "Experiment"], scale = TRUE)

# Add experiment labels to PCA results
pca_exp05_results <- data.frame(pca_exp05$x, Experiment = "exp05")
pca_exp07_results <- data.frame(pca_exp07$x, Experiment = "exp07")

# Combine the PCA results
combined_pca_results <- rbind(pca_exp05_results, pca_exp07_results)

```

# Results

\[\~200 words\]

Tables and figures (maps and other graphics) are carefully planned to convey the results of your analysis. Intense exploration and evidence of many trials and failures. The author looked at the data in many different ways before coming to the final presentation of the data.

Show tables, plots, etc. and describe them.

```{r}
# Viewing the PCA Loadings of all the variables and files 
# Extract the loadings from the PCA result
# Plot PCA for exp05
# Create a data frame with the PCA results and the experiment labels
pca_results <- data.frame(pca_combined$x, Experiment = combined_df$Experiment)

# Plot PCA using factoextra
fviz_pca_ind(pca_combined,
             geom.ind = "point", 
             col.ind = pca_results$Experiment, # color by experiment types
             palette = c("#00AFBB", "#FC4E07"),
             addEllipses = TRUE, 
             legend.title = "Experiment")

# Plot PCA using ggplot2 for custom visualization
pca_plot_df <- data.frame(PC1 = pca_combined$x[,1], PC2 = pca_combined$x[,2], Experiment = combined_df$Experiment)

ggplot(pca_plot_df, aes(x = PC1, y = PC2, color = Experiment)) +
  geom_point(alpha = 0.7) +
  stat_ellipse(level = 0.95) +
  labs(title = "PCA of Combined Experiment Data",
       x = "Principal Component 1",
       y = "Principal Component 2") +
  theme_minimal()
```

```{r, fig.width=6, fig.height=3, fig.cap="Map of completely random data"}
m <- leaflet(data) %>% 
  addTiles() %>% 
  addCircleMarkers(~x, ~y, radius = ~size,color = ~as.factor(category)) %>% 
  addPopups(~x[2], ~y[2], "Random popup")
m  # a map with the default OSM tile layer
```

```{r}
data %>% 
  ggplot(aes(x=x,y=y,col=category))+
  geom_point()
```

### Dygraphs Example

```{r}
library(dygraphs)
dygraph(nhtemp, main = "New Haven Temperatures") |> 
  dyRangeSelector(dateWindow = c("1920-01-01", "1960-01-01")) 
```

# Conclusions

\[\~200 words\]

Clear summary adequately describing the results and putting them in context. Discussion of further questions and ways to continue investigation.

# References

All sources are cited in a consistent manner
